[{"content":"Introduction Linear regression is one of the most useful techniques used by analytical chemists, since it allows us to find relate a response variable $Y$ with a controlled variable $X$ through a mathematical expression:\n$$ Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, i = 1,\u0026hellip;,n, $$\nwhere:\n $\\beta_0$ is the intercept, $\\beta_1$ the slope $\\varepsilon_i$ the error in the measurement.  As you can see, the previous equation has the subindex $i$, which means that each measured data point $Y_i$ can be associated with a data point $X_i$. Instead of having $n$ equations (one for each data point), we can write the previous expression in matrix notation:\n$$ \\mathbf{Y} = \\mathbf{X \\beta} + \\mathbf{\\varepsilon} $$\nWhere $\\mathbf{Y, X, \\beta}$ and $\\mathbf{\\varepsilon}$ are vectors containing $n$ elements. This equation can also be written as:\n$$ \\begin{pmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\Y_n\\end{pmatrix} = \\begin{pmatrix}1 \u0026amp; X_1\\\\ 1 \u0026amp; X_2\\\\ \\vdots\\\\1 \u0026amp; X_n\\end{pmatrix} \\begin{pmatrix}\\beta_0\\\\ \\beta_1 \\end{pmatrix} $$\nIt\u0026rsquo;s worth noting that the column of 1 in $\\mathbf{X}$ is used for the matrix multiplication in order to obtain the intercept $\\beta_0$. An huge advantage of using the linear algebra notation is that it allows us to generalize the previous equation to any number of predictor variables X (as long as this number is smaller than the number of observations). To do this, a column to the design matrix is added per each extra variable. These variables can be new physical quantities or they can be one of the previous after being transformed in some way by applying a function to them (e.g. $X^2$). This way, the equation would be:\n$$ \\begin{pmatrix}Y_1\\\\ Y_2\\\\ \\vdots\\\\Y_n\\end{pmatrix} = \\begin{pmatrix}1 \u0026amp; X_{1,1} \u0026amp; X_{1,2} \u0026amp; \\dots \u0026amp; X_{1,K} \\\\ 1 \u0026amp; X_{2,1} \u0026amp; X_{2,2} \u0026amp; \\dots \u0026amp; X_{2,K} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 1 \u0026amp; \\dots \u0026amp; \\dots \u0026amp; \\dots \u0026amp; X_{N,K}\\end{pmatrix} \\begin{pmatrix}\\beta_0\\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_N\\end{pmatrix} $$\nIn order to perform a regression, we need to minimize the residuals $(Y - \\hat{Y})^2$, where $Y$ is the measured value and $\\hat{Y}$ is the predicted value, this can also be expressed as $(\\mathbf{Y} - \\mathbf{X}\\beta)^T(\\mathbf{Y} - \\mathbf{X}\\beta)$. To obtain the smallest residual, we need to derivative this expression with respect to $\\beta$ and equal it to 0.\n$$ \\frac{\\partial}{\\partial \\beta} (\\mathbf{Y} - \\mathbf{X}\\beta)^T(\\mathbf{Y} - \\mathbf{X}\\beta) = 0 $$\nAfter deriving the previous equation and solving for $\\beta$ (the proof is left as an exercise for the reader), we obtain:\n$$ \\hat{\\beta} = (\\mathbf{XX}^T)^{-1}\\mathbf{X^TY} $$\nNote that $\\hat{\\beta}$ is the particular set of coefficients that minimizes the residuals (not any $\\beta$ !).\nCase study In chemometrics (and other areas) it\u0026rsquo;s necessary to implement linear regressions to predict the behavior of a system or as calibration curves. In the paper titled \u0026ldquo;Quantitative structure-property relationships prediction of some physico-chemical properties of glycerol based solvents\u0026rdquo; by Pascual Pérez et. al., the authors predicted physico-chemical properties of these solvents in order to facilitate the search of possible applications for them.\nThe authors used 27 topological parameters such as hydrogen bond acceptor counters (HBA), hydrogen bond donor counters (HBD) and rotatable bond counters (RB), between others, to predict the solvent polarity, viscosity and boiling point of the compounds.\nThe main benefit of this approach is that it allows to determine physico-chemical properties of compounds with their structure rather than performing costly or time-consuming experiments to measure them.\nFor this project, the data set provided by the authors is a matrix of 27 columns (26 topological parameters + boiling point) and 62 rows (glycerol solvents). After separating the Y variable from the X matrix, it\u0026rsquo;s possible to work with it.\nOne of the main limitations of linear regression is collinearity, this means that 2 or more predictor variables are highly correlated between each other, sometimes making impossible to obtain the regression coefficients.\nAs a way to \u0026ldquo;clean\u0026rdquo; the data set, I decided to calculate the correlation matrix, and when I found a pair of predictor variables with a correlation of 0.99 (or higher), I deleted the variable that had lower correlation with Y.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Checking for correlation corr \u0026lt;- round(cor(data),5) highcorr \u0026lt;- c() cutoff \u0026lt;- 0.99 #Deleting highly correlated data for(i in 1:(ncol(corr)-2)) for(j in (i+1):(nrow(corr)-1)) if(abs(corr[i,j]) \u0026gt;= cutoff) { if(abs(corr[nrow(corr),j]) \u0026gt;= abs(corr[nrow(corr),i])) highcorr \u0026lt;- append(highcorr,i) else highcorr \u0026lt;- append(highcorr,j) } highcorr \u0026lt;- unique(highcorr) data \u0026lt;- data[,-highcorr]   Now we are good to go!\nGenetic algorithms Choosing the right combination of predictor variables is not an easy task. Some people recommend step-wise regression, which basically consists of making a model with all the variables (in the case of backward regression) and delete the one that has the least statistically significant coefficient, calculating the model again and stopping the process until all the coefficients are considered statistically significant. This process has severe draw backs, like not considering the type-I error from repeated statistical tests, and it\u0026rsquo;s not recommended.\nIn this project, I chose a different approach to solve this problem. A genetic algorithm is a method used for optimizing a result based on the principles of genetics. As general overview, the problem must be coded as a chromosome (a string of 1s and 0s), and each of its elements it\u0026rsquo;s called a gene. The algorithm is initialized by generating a determined number of chromosomes randomly. These chromosomes or individuals then \u0026ldquo;breed\u0026rdquo; between each other to create a new generation. During breeding, 2 things happen:\n Cross-over: A son is generated by mixing one part of a parent\u0026rsquo;s chromosome with the remaining part of the other parent\u0026rsquo;s (i.e. if a chromosome has 10 genes, a way of cross-over would be combining the 4 first genes of parent 1 with the last 6 genes of parent 2). Mutation (Bit-flip): After cross-over, a mutation can occur in one (or more) of the genes. The mutation consists of flipping a bit (changing a 1 for a 0 or vice-versa) with a low probability. In this case, each gene has a 1% chance of mutation.  Also, in order for the algorithm to converge faster into the best result, a process called \u0026ldquo;elitism\u0026rdquo; can be performed. In this project, the best 2 individuals automatically pass to the next generation and ALSO, the best individual is guaranteed to breed with any of the other individuals of its generation.\nNow, how do we go from our matrix $\\mathbf{X}$ to a chromosome? Easy, we make a vector with 1s and 0s, with a length of the number of columns in $\\mathbf{X}-1$, which is the number of variables in our model. For example, if we our model had 3 predictor variables, and we have a chromosome $(1~0~1)$ this would mean we would use variables 1 and 3 for the model, but we wouldn\u0026rsquo;t include variable 2.\nCross-over and mutation algorithms were coded in R:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  bit_flip \u0026lt;- function(bit) { if(bit == 0) bit = 1 else bit = 0 } #Bit flip mutate \u0026lt;- function(parent,mutation_pool) { mutant \u0026lt;-vector(\u0026#34;numeric\u0026#34;, length(parent)) repeat { for(i in 1:length(parent)) { if(sample(mutation_pool,1) == 1) mutant[i] \u0026lt;- bit_flip(parent[i]) else mutant[i] \u0026lt;- parent[i] } if(sum(mutant) \u0026gt; 0) #Prevent a chromosome from containing only 0s break } return(mutant) }   1 2 3 4 5 6 7 8 9 10 11 12 13 14  crossover \u0026lt;- function(parent1, parent2) { crossover_child \u0026lt;- vector(\u0026#34;numeric\u0026#34;, length(parent1)) splice_index \u0026lt;- 2:(length(parent1)-1) #Don\u0026#39;t include element 1 or last  repeat { crossover_gene \u0026lt;- sample(splice_index,1) crossover_child \u0026lt;- append(parent1[1:crossover_gene],parent2[(crossover_gene+1):length(parent2)]) if(sum(crossover_child) \u0026gt; 0) break } return(crossover_child) }   And the chromosomes were generated as follows:\n1 2 3 4 5 6 7 8 9  generate_chromosome \u0026lt;- function(no_genes) { repeat{ chromosome \u0026lt;- sample(bits,no_genes, replace=TRUE) if(sum(chromosome \u0026gt; 0)) break } return(chromosome) }   In the previous code, the lines that include the function called sample are used to simulate the chance of mutation, if a 1 is selected from a vector with 99 0s, and a 1, a bit flip will occur.\nIn order to apply the model to a chromosome, we need to take a single chromosome, look at the positions of the 1s, store these indexes, and extract the corresponding columns from the original data. This data sub-set is then used to regress the model.\n1 2 3 4 5  expressed_genes \u0026lt;- which(chromosome_matrix[,j] == 1) expressed_genes \u0026lt;- append(expressed_genes, ncol(data)) #Adding Y to the data set chromosome \u0026lt;- data[,expressed_genes] model \u0026lt;- lm(b.p. ~ ., data=chromosome)   Then, the model is obtained for each chromosome, we need to find a mathematical way to express which individuals are best. For this I used the adjusted $R^2$ to select the variables that best predicted the boiling point but with a small trick, I penalized the model for including redundant information by calculation the Variance Inflation Factor (VIF). The resulting fitness function is:\n1  f \u0026lt;- summary(model)$adj.r.squared -mean(vif(model))   Finally, after calculating the fitness of all the chromosomes, the best two are selected for the next generation, and then, crossover (and mutation) kick in in order to generate the next generation. Then this whole process was repeated a few hundreds of times.\nResults The best model obtained after ~200 generations is summarized:\n Fitness function of the best chromosome per generation:   The best model was obtained by using 3 predictor variables: hydrogen bond acceptor counters (HBA), hydrogen bond donor counters (HBD) and rotatable bond counters (RB). It\u0026rsquo;s worth noting that before penalizing the model for collinearity, another model with 5 variables was found, whose biggest VIF was around 20 (a VIF of \u0026gt;5 already tells us about a problem), as a comment, the model found by the genetic algorithm is the same the authors reported in the original article.\nWe can observe that the best model was found in less than 200 generations (the original simulation was run with 1500 generations), considering that each generation contained 10 chromosomes, the result was obtained by trying 2000 models out of ~260 000 different combinations, this means our genetic algorithm could find the best result by scanning less than 1% of the total amount of models!\nFinally, in order to evaluate the model, an external validation was done by dividing the data into a training set (70% of data) and a test set (30% of data).\nStandardized residuals of training and test sets.:   It\u0026rsquo;s possible to see that our training set has one extreme and one moderate outlier, while the test set has none. This may suggest that it\u0026rsquo;s necessary to perform an outlier analysis in the original data.\nConclusions The study of molecules through their topological parameters and multi-linear regression is a powerful approach that allows to predict physico-chemical properties without performing experiments, but choosing the right variables is not an easy task. Genetic algorithms provide a way to find the best parameters without trying all combinations or doing stepwise regression, in this project, the proposed GA was able to find the model the authors proposed in less than 200 generations. A different approach to find a model, would be to use PCR or PLS to perform the regression, allowing us to obtain better results with a small amount of variables and overcoming the problem of collinearity.\n","description":"By applying multi-linear regression on topological parameters of molecules, it's possible to determine certain physico-chemical properties without experiments.","id":2,"section":"posts","tags":["Multi-linear regression","Genetic algorithms"],"title":"Use of multi-linear regression and genetic algorithms to predict the boiling point of glycerol based solvents","uri":"https://adrian-venegasreynoso.github.io/AdrianVenegasReynosoChemometrics.github.io/en/posts/genetic/"},{"content":"Overview Near-Infrared is the region of the electromagnetic that spans from 780 nm to 2500 nm.\nFor a long time, it was regarded as the \u0026ldquo;garbage can\u0026rdquo; of spectroscopy, since spectral bands in this region are\nbroad, of low intensity and hard to interpret. In comparison to other techniques, the development of NIRS was\nmainly driven by its applications and not by the knowledge of the physical phenomena. It was first used in the\nagricultural field in 1964 to determine the moisture content in grains, causing an interest on this technique to\ndetermine other analytes, such as fats and proteins.\nFurthermore, the availability of computers able to perform multi-variate analysis made NIRS one of the preferred\ntechniques to perform fast, non-destructive and precise analysis.\nCase study In order to measure the solid content in fruits (expressed as ºBrix), the NIR spectra of some fruit was measured.\nThe provided data set consists of 3 files:\n Calibration set: The spectra of 1066 samples, obtained with the same instrument (instrument 1) is presented, along with the sugar content in ºBrix. Standardization set: Contains the spectra of 245 samples, the measurements were repeated with 7 different instruments (instrument 1 to 7), for a total of 1715 spectra. Validation set: Contains the spectra of 993 samples, the measurements were performed with all the instruments, but there\u0026rsquo;s no repeated measurements.  The data is organized in such a way that every row is a sample and every column is a wavelength, in this case, there\u0026rsquo;s 256 wavelenghts in the range of 300-1100 nm.\nThe idea is to use the calibration set to make and validate a predictive model, then use the standardization test\nto take into account the differences between instruments through calibration transfer and finally, predict the data present in the validation set.\nAs a note, the data was provided by Simon Kollart, from the Federal Department of Economic Affairs, and be found at: https://chemom2021.sciencesconf.org/resource/page/id/5\nProcedure The followed approach consists of the following steps:\n Data visualization Data pre-processing Data modeling Model validation Calibration transfer Final prediction  Data visualization The first step of any data analysis should always visualize the data in order to be able to obtain information from it, for example, we can observe if the spectra is noisy, if there are problems with the base line or if there\u0026rsquo;s some region of the spectra that may be of particular interest, all this information obtained from a visual analysis also gives hints of which pre-treatments will be necessary to apply.\nNIR spectra of 1066 fruits: Spectra from 300 to 1100 nm   As it\u0026rsquo;s possible to observe on the previous image, the spectra is very noisy on the region of 300-350 nm, we can\nalso observe the spectra have the different baselines and we can see one spectra (in green) that is an outlier.\nIn order to know which pre-treatments will be useful we need to visualize the scatter effects by averaging all\nthe spectra by wavelength (in our data, it would be taking the mean by column) and plotting it against the\nindividual value of each spectra.\nScatter effects: Scatter effects of the spectra obtained by plotting the mean of each column of the matrix vs each value of its column. The parallel lines suggest the presence of additive scatter effects.   Parallel lines suggest the presence of additive scatter effects, while lines that intersect at some point but have different slopes show the presence of multiplicative scatter effects. In this case, the observed additive scatter effects\ncan be easily corrected by taking the 1st or 2nd derivative of the spectra.\nData pre-processing From the previous step, we got a rough overview of the quality of the data, and it was possible to observe some noisy regions and the presence of additive scatter effects, both of this problems can be easily solved by using the Savitzky-Golay filter, which consists of a moving window that smooths the signal (by fitting a polynomial) and then takes the derivative of this function. But, just before doing this, I applied the SNV pre-processing filter, which consists of taking centering each spectra by its mean and dividing by its standard deviation (it can be seen as an auto-centering by spectra, not by variable), since from experience, I know I will get a better result later on.\nAfter applying both SNV and Savtizky-Golay filter (21 points, 2nd order polynomial and 2nd order derivative), the plot of scatter effects looks as follows:\nScatter effects of pre-processed data: The scatter effects have been greatly reduced after applying the filters to the raw data   After applying these filters, the spectra looks as follows:\nProcessed spectra: SNV and Savtizky-Golay filter, with 21 points, 2nd order polynomial and 2nd derivative.   As we can see, the effect of the baseline is not present and the spectra looks smoother. Now, time to modeling.\nData modeling First, we need to look at the data from a chemist point of view; the region of the spectra from 500 to ~690 nm is where chlorophyll absorbs visible light, which correlates with the color of the fruit, then from 750 to 950 nm, there\u0026rsquo;s the region of the spectra where proteins and sugar absorb radiation, which is related to what we want to measure. Finally, we can see the influence of water in the region above 950 nm.\nThis data can also be analyzed from a statistics point of view, by performing iPLS (interval Partial Least Squares), an algorithm that splits the data into several regions and performs PLS on them, then, it combines the models from\nthese intervals and evaluates if the change on the error of the resulting model is statistically significant. This process is repeated several times until all the intervals have been combined.\nIn R, this can be easily performed with the mdatools library, for this the following code was used, where the spectra was divided into 32 internal segments and 7 global segments.\n1 2 3 4  library(mdatools) intervals \u0026lt;- ipls(spectra, y_response, glob.ncomp=7, int.num=32, method=\u0026#34;forward\u0026#34;) summary(intervals) plot(intervals)   iPLS: The plot shows the error of the cross-validation, obtained by adding each segment of the spectra to the model, in green we can observe the regions of the spectra that provide an statistical improvement to the model.   From this plot, we can see that the variables with an index from ~150 to ~190, which translate to a region of the spectra from ~740 to ~950 nm. The chemical and the statistical agree on which part of the spectra is useful for developing the model, obviously, the starting and ending wavelengths can be fine-tuned later by monitoring the error of the validation of the model.\nJust as a very brief summary, first we pre-process the data with SNV and Savtizky-Golay filter, and then, the area of interest of the spectra was chosen from the pre-processed data.\nNow, to develop the model, the Partial Least Squares method (PLS) was used, in R this can be easily implemented with the pls library:\n1 2 3 4 5 6 7  library(pls) pls.options(parallel=6) ncomp \u0026lt;- 10 spectra_data \u0026lt;- data.frame(y_response=y_response, NIR=I(spectra)) pls_model \u0026lt;- plsr(y_response ~ NIR, data=spectra_data, ncomp=ncomp)   From the previous code, first the library pls is loaded, (let\u0026rsquo;s ignore the second line for now), then, ncomp stands for \u0026ldquo;number of components\u0026rdquo; or latent variables that were used to develop the model, the spectra and the value of the reference method (ºBrix) are neatly organized into a data frame and the model is built with the plsr function.\nWe can look at the scores plot by graphing the latent variable 1 against the latent variable 2 with ggplot2.\n1 2 3 4 5 6 7 8  library(ggplot2) scores_data \u0026lt;- data.frame(LV1=pls_model$scores[,2],LV2=pls_model$scores[,3], LV3=pls_model$scores[,3],Y=y_response) scores_data \u0026lt;- data.frame(LV=I(pls_model$scores), Y=y_response) scores \u0026lt;- ggplot() + geom_point(data=scores_data, aes(x=LV[,1], y=LV[,3], colour=Y)) + scale_color_gradientn(colors=rainbow(5)) plot(scores)   Scores plot: Scores plot of latent variable 1 vs latent variable 2.   As it\u0026rsquo;s possible to observe, when we plot LV 1 vs LV2, samples are localized in the plane according to their ºBrix (in the plot, it\u0026rsquo;s named as \u0026ldquo;Y\u0026rdquo;), which shows that the model is able to discriminate according to the content of sugars. More analysis can be done from the results, such as looking at the loadings and scree plots, but in attempt to keep this post as short as possible, these steps will be omitted.\nModel validation A model is meaningless if it\u0026rsquo;s not validated. Even though it is possible to calculate the R2 from our model, this value can be meaningless if we over-fit our data. This means that our model can be extremely good at predicting the data it was provided with, but it has a bad performance at predicting new data, which in the end, is the purpose of making a model.\nSome of the most common ways of validating a model are:\n Internal calibration: A common algorithm is the Leave One Out (LOO), which consists of making the model with all the data except a single one, which will be predicted and then the error is calculated. The process is repeated for every point in the data set. External calibration: Before starting, the data set is divided into a calibration and a test set, the first contains around ~70% of the data while the latter has ~30%. The model is regressed on the calibration set and then the test set is predicted.  Internal calibration usually ends up in over-confident estimates of the model, while external calibration is very susceptible to luck, why? Because we could select a test set that can be particularly easy or difficult to predict.\nIn order to overcome these limitations, it is possible to perform a cross-validation algorithm known as k-folds, where the data set is divided in k segments, one of the segments is left out and the regression model is built with the others, then the excluded segment is predicted and the error of the prediction calculated, the process is repeated until every segment has been left out, and the errors are averaged. R has perform these calculations with the pls and parallel libraries, the latter package allows to use different CPU cores in order to complete the calculations faster.\n1 2 3 4 5 6  pls.options(parallel=6) ncomp \u0026lt;- 10 spectra_data \u0026lt;- data.frame(y_response=y_response, NIR=I(spectra)) pls_model \u0026lt;- plsr(y_response ~ NIR, data=spectra_data, ncomp=ncomp)   Since we are looking at spectroscopic data, a very useful approach to select the number of latent variables is to look at the loadings plots (loadings vs wavelength), when these graphs stop to resemble the original spectra and start showing noise, it\u0026rsquo;s a signal to stop adding variables to our model.\nLoadings plot: The first plot corresponds to the pre-processed data on the selected region of the spectra, the following graphs show the loadings of latent variables 1, 2, 5,9 and 10, as it\u0026#39;s possible to see, the loadings resemble the original data, but by LV 10, some noise starts to appear.   In this case, 9 latent variables were considered to be an adequate amount of components for the model.\nCalibration transfer Sometimes different instruments will be needed to perform the measurements in field, but a problem arises from the differences between them, since they may obtain different signals from the sample sample (wavelength and intensity shifts). A way to overcome this problem without repeating the calibration on all the instruments (which is an expensive and time demanding procedure) it is possible to transfer the calibration between instruments. This process consists on selecting a smaller number of samples which will be measured with all the available instruments, including the one which was used to build the model (master instrument). In general terms, the calibration transfer is done by converting the spectra obtained on the same sample by the \u0026ldquo;slave\u0026rdquo; instruments to the signal the \u0026ldquo;master\u0026rdquo; instrument measures. A popular algorithm to perform such task is Piecewise Direct Standardization, the method uses PLS to build a model to predict the master\u0026rsquo;s instrument spectra from the slave\u0026rsquo;s instrument one. A very useful explanation is available at: https://guifh.github.io/RNIR/PDS.html. For this project, I made a function (transfer.R) that separated the data from the standardization set into the slave and master data, the data was pre-processed as it has been previously mentioned and then, PDS was applied to each of the 6 slave\u0026rsquo;s instrument data, finally, the obtained models are used to transform the spectra from the validation set into \u0026ldquo;instrument 1\u0026rdquo; data.\nPiecewise Direct Standardization: The blue spectra come from the pre-processed data from the master instrument, in green we can observe the data of the spectra obtained with a slave instrument and in red, we can see the spectra after it has been standardized, it\u0026#39;s possible to see the spectra resembles the master\u0026#39;s instrument data more.   Final prediction Final prediction is applied to the validation set after it has been pre-processed and standardized to the master instrument.\n1  predictions \u0026lt;- predict(pls_model, ncomp=ncomp, newdata=validation)   After this, we have a vector with 993 values containing the sugar content in ºBrix,\nReference vs predicted values: Scatter plot showing the relation between reference and predicted values, perfect prediction is showed with the red line.   The error of the model, expressed is:\n$$ RMSEP = \\sqrt{\\frac{\\sum_i^n(y_i - \\hat{y_i})^2}{n}} = 0.79$$\nWhere $n$ is the number of samples, $y_i$ is the $i$-th reference value and $\\hat{y_i}$ is its prediction. The relative error of the model is around 5.6%.\nAnother way to assess the quality of the model is through the RDP, which is the standard deviation of the reference values divided by the RMSEP of the model.\nThe RDP can be classified as:\n 1.5 - 2: The model can discriminate between high and low range values. 2 - 2.5: Rough predictions. 3: Good. 4: Excellent.  For this case, an RDP of 2.36 was obtained.\nConclusions PLS was used to predict the sugar content in fruits from its NIR spectra. SNV and Savitzky-Golay filter (21 points, 2nd order polynomial and 2nd derivative) where used to treat the data, also the spectral region 750-950 nm was used.\nValidation of the model was done with cross-validation (k-folds) and by looking at the loadings plot, 9 latent variables were chosen. Then, Piecewise Direct Standardization was used to transfer the calibration between 6 slave instruments. Finally, the obtained model was applied on the pre-processed and standardized data, obtaining a relative RMSEP of 5.6% and an RDP of 2.36, meaning that the obtained model can only be used to make rough predictions. The model could be improved by exploring other pre-processing techniques such as (Extended) Multiplicative Scatter Correction, also, other algorithms to perform the regression can be used, such as Support Vector Machines and Artificial Neural Networks.\n","description":"The combination of NIR spectroscopy and chemometrics allows us to measure the content of sugar in fruits in a fast, non-destructive manner.","id":3,"section":"posts","tags":["hugo"],"title":"Use of Near-Infrared (NIR) Spectroscopy to measure the sugar content in fruits","uri":"https://adrian-venegasreynoso.github.io/AdrianVenegasReynosoChemometrics.github.io/en/posts/nir/"},{"content":"My name is Adrián Venegas Reynoso, and I\u0026rsquo;m a Masters student in industrial analysis at Université Claude Bernard Lyon 1. I have a passion for programming, statistics and chemistry and I think chemometrics is a field that combines these areas, allowing to make real life decisions through modelling.\nYou can see some the complete code I use for my projects at my github repository.\n https://github.com/adrian-venegasreynoso/projects  I hope you take a read and enjoy my blog!\n","description":"My chemometrics portfolio","id":8,"section":"","tags":null,"title":"About","uri":"https://adrian-venegasreynoso.github.io/AdrianVenegasReynosoChemometrics.github.io/en/about/"}]